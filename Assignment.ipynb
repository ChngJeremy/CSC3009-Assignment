{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision import models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check for GPU Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce GTX 1070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") # use gpu if available\n",
    "# print device name if using gpu with message \"Using GPU: device_name\"\n",
    "if device.type == \"cuda\":\n",
    "    print(\"Using GPU: {}\".format(torch.cuda.get_device_name(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Split the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # normalize in range [-1,1]\n",
    "])\n",
    "\n",
    "# Define a function to load and preprocess images\n",
    "def load_and_preprocess_image(file_path):\n",
    "    # Open the image file\n",
    "    with Image.open(file_path) as img:\n",
    "        # Apply the transformations\n",
    "        img_transformed = transform_train(img)  # use transform_train instead of transform\n",
    "    return img_transformed\n",
    "\n",
    "# Define the directories where your images are stored\n",
    "covid_dir = \"./dataset_14/covid\"\n",
    "normal_dir = \"./dataset_14/normal\"\n",
    "\n",
    "# Load and preprocess all images\n",
    "data = []\n",
    "labels = []\n",
    "for filename in os.listdir(covid_dir):\n",
    "    img_tensor = load_and_preprocess_image(os.path.join(covid_dir, filename))\n",
    "    data.append(img_tensor)\n",
    "    labels.append(1)  # '1' for 'covid'\n",
    "for filename in os.listdir(normal_dir):\n",
    "    img_tensor = load_and_preprocess_image(os.path.join(normal_dir, filename))\n",
    "    data.append(img_tensor)\n",
    "    labels.append(0)  # '0' for 'normal'\n",
    "\n",
    "# Convert the data and labels to PyTorch tensors\n",
    "data = torch.stack(data)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# First, split the data into a training set and a temporary set using an 80-20 split\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the temporary set into a validation set and a test set using a 50-50 split\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset = TensorDataset(val_data, val_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32 # Parameter: batch size\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # conv1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),  # Changed in_channels to 1\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2),\n",
    "            # conv2\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2),\n",
    "            # conv3\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2),\n",
    "            # conv4\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.dense_layers = nn.Sequential(\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(50176, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(1024, 2),  # Changed K to 2\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.conv_layers(X)\n",
    "\n",
    "    # Flatten the output of the conv layers\n",
    "        out = out.view(out.size(0), -1)\n",
    "    \n",
    "    # Fully connected\n",
    "        out = self.dense_layers(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "net = CNN()\n",
    "net = net.to(device) # move the network to GPU if available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning with VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the validation images: 81 %\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained VGG16 model\n",
    "net = models.vgg16(pretrained=True)\n",
    "\n",
    "# Modify the last layer\n",
    "num_features = net.classifier[6].in_features\n",
    "net.classifier[6] = nn.Linear(num_features, 2)  # '2' for 'covid' and 'normal'\n",
    "\n",
    "# Move model to GPU if available\n",
    "net = net.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# initialize tensorboard\n",
    "writer = SummaryWriter('runs/covid_classifier_experiment_1')\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 20 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                (epoch + 1, i + 1, running_loss / 20))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # log the epoch loss and accuracy\n",
    "    writer.add_scalar('Loss/train', running_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', 100 * correct / total, epoch)\n",
    "\n",
    "# Accuracy test and image comparison on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Add image comparisons to TensorBoard\n",
    "        img_grid = torchvision.utils.make_grid(images)\n",
    "        writer.add_image('covid_vs_normal', img_grid)\n",
    "\n",
    "print('Accuracy of the network on the validation images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the network on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the validation images: 72 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Accuracy test and image comparison on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "        for data in val_dataloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Add image comparisons to TensorBoard\n",
    "        img_grid = torchvision.utils.make_grid(images)\n",
    "        writer.add_image('covid_vs_normal', img_grid)\n",
    "\n",
    "        writer.add_scalar('Accuracy/validation', 100 * correct / total, epoch)\n",
    "        writer.add_scalar('Loss/validation', running_loss, epoch)\n",
    "\n",
    "print('Accuracy of the network on the validation images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
